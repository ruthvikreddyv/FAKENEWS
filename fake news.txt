#importing libraries

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

#dataset

data = pd.read_csv('News.csv',index_col=0) # the first column should be used as the index

#size of the dataset 

data.shape

# Dropping the "title", "subject", and "date" columns from the DataFrame, 
# keeping only the relevant data (e.g., "text" column) for further processing.

data = data.drop(["title", "subject", "date"], axis=1)
data.isnull().sum() #missing (null) values in each column of a Pandas DataFrame.

# Shuffling
 
data = data.sample(frac=1)
data.reset_index(inplace=True)
data.drop(["index"], axis=1, inplace=True)

# Creating a count plot to visualize the distribution of the 'class' column values, 
# with bars ordered by the frequency of occurrences in descending order.

sns.countplot(data=data,
                x='class',
              order=data['class'].value_counts().index)

#downloading packages and importing libraries

from tqdm import tqdm
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud

# Function to preprocess text data by removing punctuation, converting to lowercase, 
# and removing stopwords. Uses a progress bar (tqdm) to show the loop's progress.
def preprocess_text(text_data):
    preprocessed_text = []
    
# Loop over each sentence in the input text data
 for sentence in tqdm(text_data):
# Remove punctuation from the sentence
 sentence = re.sub(r'[^\w\s]', '', sentence)
        
# Convert to lowercase, remove stopwords, and append the processed sentence to the list
    preprocessed_text.append(' '.join(token.lower()
                            for token in str(sentence).split()
                               if token not in stopwords.words('english')))
    
 return preprocessed_text

# Preprocess the 'text' column in the DataFrame by applying the preprocess_text function.
# This cleans the text data by removing punctuation, converting to lowercase, 
# and removing stopwords. The cleaned text is then reassigned to the 'text' column.
preprocessed_review = preprocess_text(data['text'].values)
data['text'] = preprocessed_review


# Generate and display a word cloud for the text data where 'class' == 1 (real news).
# The text is consolidated into a single string, and a word cloud is created
# with specific size, font, and layout parameters.



consolidated = ' '.join(
	word for word in data['text'][data['class'] == 1].astype(str))
wordCloud = WordCloud(width=1600,
					height=800,
					random_state=57,
					max_font_size=110,
					collocations=False)
plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()


# Generate and display a word cloud for the text data where 'class' == 0 (fake news).
# The text is consolidated into a single string, and a word cloud is created
# with specific size, font, and layout parameters.
consolidated = ' '.join(
    word for word in data['text'][data['class'] == 0].astype(str))
wordCloud = WordCloud(width=1600, height=800, random_state=57, max_font_size=110, collocations=False)
plt.figure(figsize=(15, 10))
plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')
plt.axis('off')
plt.show()

# Function to extract the top 'n' most frequent words from a given text corpus using CountVectorizer.
def get_top_n_words(corpus, n=None):
    vec = CountVectorizer().fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    
    # Create a list of words with their frequencies
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    
    return words_freq[:n]

# Extract the top 20 most common words from the 'text' column in the dataset
common_words = get_top_n_words(data['text'], 20)

# Convert the common words into a DataFrame
df1 = pd.DataFrame(common_words, columns=['Review', 'count'])

# Plot the frequency of the top words as a bar chart
df1.groupby('Review').sum()['count'].sort_values(ascending=False).plot(
    kind='bar',
    figsize=(10, 6),
    xlabel="Top Words",
    ylabel="Count",
    title="Bar Chart of Top Words Frequency"
)

#importing libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

# Split the dataset into training and testing sets (75% training, 25% testing)
x_train, x_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.25)


from sklearn.feature_extraction.text import TfidfVectorizer

vectorization = TfidfVectorizer()
x_train = vectorization.fit_transform(x_train)
x_test = vectorization.transform(x_test)


from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(x_train, y_train)

# testing the model
print(accuracy_score(y_train, model.predict(x_train)))
print(accuracy_score(y_test, model.predict(x_test)))


from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

# testing the model
print(accuracy_score(y_train, model.predict(x_train)))
print(accuracy_score(y_test, model.predict(x_test)))



from sklearn import metrics
# Generate and display a confusion matrix for the Decision Tree classification results
cm = metrics.confusion_matrix(y_test, model.predict(x_test))  # Compute the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[False, True])  # Prepare the display
cm_display.plot()  # Plot the confusion matrix
plt.show()  # Show the plot


